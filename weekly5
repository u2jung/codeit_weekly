# 지도학습과 비지도 학습의 차이
지도학습 
  - 입력과 정답이 함께 주어짐
  - 주어진 입력에 대해 정답을 예측하는 모델 학습
  - 예시 : 분류, 회귀
  - 대표 알고리즘 : SVM, 결정트리, 선형회귀, 신경망등

비지도 학습
  - 입력데이터만 존재, 정답 없음
  - 데이터의 구조나 패턴을 파악
  - 예시 : 군집화, 차원축소
  - K-means, PCA, DBSCAN등

# 손실 함수(Loss Function)란 무엇이며 왜 중요한가요?
  - 손실 함수는 *모델의 예측값과 실제 정답 사이의 차이를 수치로 나타낸 함수*
중요한 이유
  - 모델이 얼마나 잘 학습 되고 있는지를 측정하는 기준
  - 학습과정에서는 이 손실을 최소화하기 위해 모델의 파라미터를 조정
  - 손실 함수가 제대로 정의되어 있지 않으면, 모델이 잘못된 방향으로 학습 될 수 있음

# 모델 학습 시 발생 할 수 있는 편향과 분산에ㅜ 대해 설명하고, 두 개념의 관계에 대해 설명해 주세요
  - 편향(Bias)
    - 모델이 단순하여 학습 데이터의 복잡한 패턴을 잘 포착하지 못하는 경우
    - 과도한 일반화 -> 과소적합(Underfitting)발생 가능.
  - 분산(Variance)
    - 모델이 지나치게 복잡해서 학습 데이터에 너무 민감하게 반응하는 경우
    - 데이터의 작은 노이즈까지 학습 -> 과적합(Overfitting)발생 가능
  - 관계 Bias-Variance Tradeoff
    - 일반적으로 편향이 낮아지면 분산이 높아지고, 반대로 편향이 높아지면 분산이 낮아지는 경향이 있음
    - 좋은 모델은 적절한 편향과 분산의 균형을 유지

# K-폴드 교차 검증에서 K의 값을 선택할 때 고려해야 할 점은 무엇인가요?
  - 데이터를 K개의 부분으로 나눠, 모델을 K번 학습/평가하여 일반화 성능을 추정하는 방법
  - 고려사항
    1. 데이터 크기
      - 데이터가 적으면 K를 크게 (e.g K = 10)설정하여 더 많은 학습 데이터를 확보
      - 데이터가 많으면 K를 작게 (e.g K = 5)해도 충분히 일반화 평가 가능
    2. 연산 비용
      - K가 커질 수록 모델을 더 자주 학습시켜야 하므로 계산 비용 증가
    3. 분산 vs 편향
      - K가 크면 테스트 세트 크기가 작아져 분산은 커지고 편향은 줄어듦
      - K가 작으면 테스트 세트 크기가 커져 편향은 커지고 분산은 줄어듦
    4. 특수 상황
      - 분류 문제에서 클래스 불균형이 있을 경우 Stratified K-Fold를 사용하는 것이 좋음
